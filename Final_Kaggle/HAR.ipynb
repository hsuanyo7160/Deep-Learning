{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90102,"databundleVersionId":10442789,"sourceType":"competition"},{"sourceId":206859,"sourceType":"modelInstanceVersion","modelInstanceId":176353,"modelId":198677},{"sourceId":206908,"sourceType":"modelInstanceVersion","modelInstanceId":176393,"modelId":198717},{"sourceId":209107,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":178282,"modelId":200579},{"sourceId":209111,"sourceType":"modelInstanceVersion","modelInstanceId":178286,"modelId":200583},{"sourceId":209371,"sourceType":"modelInstanceVersion","modelInstanceId":178506,"modelId":200800},{"sourceId":212269,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":180945,"modelId":203180},{"sourceId":212365,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":181028,"modelId":203255},{"sourceId":212689,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":181291,"modelId":203524},{"sourceId":214068,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":182478,"modelId":204694}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport timm\n#os.system(\"pip install --upgrade timm\")\nimport pandas as pd\nimport torch.nn as nn\nfrom torch.optim import AdamW\nimport numpy as np\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom timm.models import create_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom timm.data import Mixup\nfrom torchvision.transforms import RandomErasing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T04:42:27.458915Z","iopub.execute_input":"2024-12-31T04:42:27.459105Z","iopub.status.idle":"2024-12-31T04:42:35.102993Z","shell.execute_reply.started":"2024-12-31T04:42:27.459086Z","shell.execute_reply":"2024-12-31T04:42:35.102312Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 檢查是否有可用的 GPU，否則使用 CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T04:42:35.103777Z","iopub.execute_input":"2024-12-31T04:42:35.104146Z","iopub.status.idle":"2024-12-31T04:42:35.160235Z","shell.execute_reply.started":"2024-12-31T04:42:35.104125Z","shell.execute_reply":"2024-12-31T04:42:35.159461Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================\n# 2. 自定義數據集 (讀取訓練數據)\n# ============================\n# 定義數據集路徑\nnum_classes = 15\nIMAGE_SIZE = 336\nBATCH_SIZE = 8\nIMAGE_DIR = \"/kaggle/input/2024-deep-learning-final-project/train_images\"\nCSV_PATH = \"/kaggle/input/2024-deep-learning-final-project/train_set.csv\"\nOUTPUT_DIR = \"/kaggle/working/\"\n# 讀取 CSV 文件\ndata_df = pd.read_csv(CSV_PATH)\n\n# 添加圖片完整路徑\ndata_df['file_path'] = data_df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n\n# 創建 Label 字典\nunique_labels = data_df[['label']].drop_duplicates().sort_values('label').reset_index(drop=True)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels['label'].unique())}  # 類別 -> 數字\nreverse_label_map = {v: k for k, v in label_map.items()}  # 數字 -> 類別\n\n# 替換 DataFrame 中的文字標籤為數字\ndata_df['label'] = data_df['label'].map(label_map)\n\n# 打印 Label 字典\nprint(\"label 字典:\")\nprint(label_map)\n\n# 分割數據集\ntrain_df, val_df = train_test_split(\n    data_df, test_size=0.1, random_state=42, stratify=data_df['label']\n)\n\n# 自定義數據集類別\nclass HARImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image = Image.open(row['file_path']).convert(\"RGB\")\n        label = row['label']\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# 數據增強與處理\ntrain_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),   ###### 20->30\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomResizedCrop(size=IMAGE_SIZE, scale=(0.7, 1.0)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]), # For eva02_enormous_patch14_clip_224 #####################\n    RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #####################\n])\n\n# 創建數據集與 DataLoader\ntrain_dataset = HARImageDataset(train_df, transform=train_transform)\nval_dataset = HARImageDataset(val_df, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n# No phone: cycling dancing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T04:42:35.161011Z","iopub.execute_input":"2024-12-31T04:42:35.161210Z","iopub.status.idle":"2024-12-31T04:42:35.233600Z","shell.execute_reply.started":"2024-12-31T04:42:35.161193Z","shell.execute_reply":"2024-12-31T04:42:35.232906Z"}},"outputs":[{"name":"stdout","text":"label 字典:\n{'calling': 0, 'clapping': 1, 'cycling': 2, 'dancing': 3, 'drinking': 4, 'eating': 5, 'fighting': 6, 'hugging': 7, 'laughing': 8, 'listening_to_music': 9, 'running': 10, 'sitting': 11, 'sleeping': 12, 'texting': 13, 'using_laptop': 14}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"mixup_fn = Mixup(\n    mixup_alpha=0.2,       # MixUp 混合強度\n    cutmix_alpha=0.6,      # CutMix 混合強度\n    prob=0.7,              # 應用 MixUp/CutMix 的概率\n    switch_prob=0.5,       # MixUp 和 CutMix 之間的切換概率\n    label_smoothing=0.15,   # Label smoothing\n    num_classes=15\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T04:42:45.645832Z","iopub.execute_input":"2024-12-31T04:42:45.646115Z","iopub.status.idle":"2024-12-31T04:42:45.650148Z","shell.execute_reply.started":"2024-12-31T04:42:45.646092Z","shell.execute_reply":"2024-12-31T04:42:45.649250Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ============================\n# 3. 模型定義\n# ============================\n# swin_large_patch4_window7_224        : 91.16%\n# swin_large_patch4_window12_384       : 91.56%\n# vit_large_patch16_224                : 90.60% \n# swinv2_large_window12to16_192to256   : 91%\n# convnext_large.fb_in22k_ft_in1k_384  : 90.12%\n# convnext_xlarge.fb_in22k_ft_in1k_384 : 90.92%\n# beitv2_large_patch16_224             : 90.8%\nmodel_name = \"eva02_large_patch14_clip_336\"         \n\n# 創建模型\nmodel = create_model(model_name, pretrained=True, num_classes=num_classes)          ############# False\nmodel = model.to(device)\n\n# 查看預訓練的配置\npretrained_cfg = model.pretrained_cfg\nprint(pretrained_cfg)\n\n# ============================\n# 4. 損失函數與優化器\n# ============================\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.2)\noptimizer = AdamW(model.parameters(), lr=2e-6, weight_decay=1e-3)      # swin_large_patch4_window7_224 : 5e-5\nscheduler = CosineAnnealingLR(optimizer, T_max=25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T07:07:02.544469Z","iopub.execute_input":"2024-12-29T07:07:02.544765Z","iopub.status.idle":"2024-12-29T07:07:11.872043Z","shell.execute_reply.started":"2024-12-29T07:07:02.544733Z","shell.execute_reply":"2024-12-29T07:07:11.871294Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/856M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadd08a5035f463e8c4fa26e1282dd47"}},"metadata":{}},{"name":"stdout","text":"{'url': '', 'hf_hub_id': 'timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k', 'hf_hub_filename': 'open_clip_pytorch_model.bin', 'architecture': 'eva02_large_patch14_clip_336', 'tag': 'merged2b', 'custom_load': False, 'input_size': (3, 336, 336), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 1.0, 'crop_mode': 'center', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'num_classes': 768, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'license': 'mit'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================\n# 5. 訓練與驗證\n# ============================\nepochs = 10\nbest_accuracy = 0.0\n\nfor epoch in range(epochs):\n    # 訓練模式\n    model.train()\n    train_loss = 0.0\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        images, labels = images.to(device), labels.to(device)\n\n        # 應用 MixUp 或 CutMix\n        if mixup_fn is not None:\n            images, labels = mixup_fn(images, labels)\n\n        # 前向傳播\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    # 驗證模式\n    model.eval()\n    val_loss = 0.0\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    # 計算驗證集準確率\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}, Accuracy: {accuracy:.4f}\")\n\n    # 儲存最佳模型\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best.pth\"))\n        print(f\"Saved Best Model with Accuracy: {best_accuracy:.4f}\")\n\n    # 調整學習率\n    scheduler.step()\n\nprint(\"訓練完成，最佳模型已保存！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T12:47:10.899755Z","iopub.execute_input":"2024-12-29T12:47:10.900055Z","iopub.status.idle":"2024-12-29T12:47:15.029204Z","shell.execute_reply.started":"2024-12-29T12:47:10.900031Z","shell.execute_reply":"2024-12-29T12:47:15.027876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1 Training:   0%|          | 0/1137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2445cd7cdaef42cc83cdeef91cae3cb6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a0a682b8a4ac>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 驗證模式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# =============================================================================\n#                                微調模型\n# =============================================================================\nepochs = 10\nbest_accuracy = 0.0\n\nMODEL_NAME = \"eva02_large_patch14_clip_336\"\nMODEL_PATH = \"/kaggle/input/eva_336_0.9334/pytorch/default/1/eva336_0.933.pth\"\nmodel = create_model(MODEL_NAME, pretrained=False, num_classes=15)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\nmodel = model.to(device)\n\n# ============================\n# 1. 損失函數與優化器\n# ============================\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.15)\noptimizer = AdamW(model.parameters(), lr=5e-7, weight_decay=8e-4)      # swin_large_patch4_window7_224 : 5e-5\nscheduler = CosineAnnealingLR(optimizer, T_max=25)\n\nfor epoch in range(epochs):\n    # 訓練模式\n    model.train()\n    train_loss = 0.0\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        images, labels = images.to(device), labels.to(device)\n\n        # 應用 MixUp 或 CutMix\n        if mixup_fn is not None:\n            images, labels = mixup_fn(images, labels)\n\n        # 前向傳播\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    # 驗證模式\n    model.eval()\n    val_loss = 0.0\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    # 計算驗證集準確率\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}, Accuracy: {accuracy:.4f}\")\n\n    # 儲存最佳模型\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best.pth\"))\n        print(f\"Saved Best Model with Accuracy: {best_accuracy:.4f}\")\n\n    # 調整學習率\n    scheduler.step()\n\nprint(\"訓練完成，最佳模型已保存！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T04:42:52.264084Z","iopub.execute_input":"2024-12-31T04:42:52.264363Z","execution_failed":"2024-12-31T06:50:28.162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1 Training:   0%|          | 0/1137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d48866f0a36b4ad18102e23086b7a3da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1 Validation:   0%|          | 0/127 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9974289d42402387637061e641701d"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10, Train Loss: 1.5002, Val Loss: 0.9209, Accuracy: 0.9525\nSaved Best Model with Accuracy: 0.9525\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 Training:   0%|          | 0/1137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b187a2dc4342b4944d5d1d3307ad7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2 Validation:   0%|          | 0/127 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"679c7474452b4da49415cbe3de827ab0"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10, Train Loss: 1.5110, Val Loss: 0.9222, Accuracy: 0.9495\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 Training:   0%|          | 0/1137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4184c1e1124268827c667067162b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3 Validation:   0%|          | 0/127 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea9dbe9dcf5410a92357df4e90b74e1"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/10, Train Loss: 1.4939, Val Loss: 0.9220, Accuracy: 0.9515\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4 Training:   0%|          | 0/1137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d851e973b5884f17a13ec4618d454740"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n#                                單一模型預測\n# =============================================================================\n# ============================\n# 1. 設置參數\n# ============================\nTEST_DIR = \"/kaggle/input/2024-deep-learning-final-project/test_images\"\nOUTPUT_FILE = \"/kaggle/working/team_18_submission.csv\"\nMODEL_PATH = \"/kaggle/working/best.pth\"  # 已訓練的模型檢查點\n#model_name = \"eva02_large_patch14_clip_336\"\n\n# ============================\n# 2. 數據增強\n# ============================\ntest_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # 根據模型的輸入要求\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\n# ============================\n# 3. 加載模型\n# ============================\nprint(\"Loading model\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 創建與訓練時相同的模型\nnum_classes = len(reverse_label_map)  # 輸出類別數\nmodel = create_model(model_name, pretrained=False, num_classes=num_classes)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\nmodel = model.to(device)\nmodel.eval()\n\n# ============================\n# 4. 加載測試數據\n# ============================\n# 獲取測試圖片列表\ntest_images = sorted(os.listdir(TEST_DIR))\ntest_data = []\n\nfor img_name in test_images:\n    img_path = os.path.join(TEST_DIR, img_name)\n    img = Image.open(img_path).convert(\"RGB\")\n    img = test_transform(img)\n    test_data.append((img_name, img))\n\n# ============================\n# 5. 預測\n# ============================\nprint(\"Predicting\")\nresults = []\nwith torch.no_grad():\n    for img_name, img_tensor in tqdm(test_data, desc = \"Predicting\n    \"):\n        img_tensor = img_tensor.unsqueeze(0).to(device)  # 增加批次維度\n        output = model(img_tensor)\n        pred = torch.argmax(output, dim=1).item()  # 獲取預測類別索引\n        label = reverse_label_map[pred]  # 將索引映射為類別名稱\n        results.append({\"filename\": img_name.replace(\".jpg\", \"\"), \"label\": label})\n\n# ============================\n# 6. 保存為 CSV 文件\n# ============================\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(OUTPUT_FILE, index=False)\n\nprint(f\"預測結果已保存到 {OUTPUT_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T12:47:42.854069Z","iopub.execute_input":"2024-12-29T12:47:42.854415Z","iopub.status.idle":"2024-12-29T12:51:43.276196Z","shell.execute_reply.started":"2024-12-29T12:47:42.854386Z","shell.execute_reply":"2024-12-29T12:51:43.275459Z"}},"outputs":[{"name":"stdout","text":"Loading model\nPredicting\n預測結果已保存到 /kaggle/working/team_18_submission.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==========================================================================================================================\n#                                                            Ensemble\n# ==========================================================================================================================\n\n# ============================\n# 1. 加載模型\n# ============================\nTEST_DIR = \"/kaggle/input/2024-deep-learning-final-project/test_images\"\nprint(\"Loading models\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_model(model_name, model_path):\n    model = create_model(model_name, pretrained=False, num_classes=len(reverse_label_map))\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n    return model\n\n# 模型路徑與加載\nMODEL1_PATH = \"/kaggle/input/eva_336_0.9304/pytorch/default/1/eva02_336_0.9304.pth\"\nMODEL2_PATH = \"/kaggle/input/eva02_224_0.924/pytorch/default/1/best.pth\"\nMODEL3_PATH = \"/kaggle/input/eva_large_patch14_336_best/pytorch/default/1/eva_large_patch14_336_best.pth\"\n# eva02_large_patch14_448\nmodel1 = load_model(\"eva02_large_patch14_clip_336\", MODEL1_PATH)\nmodel2 = load_model(\"eva02_large_patch14_clip_224\", MODEL2_PATH) \nmodel3 = load_model(\"eva_large_patch14_336\", MODEL3_PATH)\n\nIMAGE_SIZE1 = 336\nIMAGE_SIZE2 = 224\nIMAGE_SIZE3 = 336\n# ============================\n# 2. 數據增強\n# ============================\ntest_transform_model1 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE1, IMAGE_SIZE1)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\ntest_transform_model2 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE2,IMAGE_SIZE2)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\ntest_transform_model3 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE3,IMAGE_SIZE3)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\n# ============================\n# 3. 加載測試數據\n# ============================\nprint(\"Preparing test data\")\ntest_images = sorted(os.listdir(TEST_DIR))\ntest_data = []\n\nfor img_name in test_images:\n    img_path = os.path.join(TEST_DIR, img_name)\n    img = Image.open(img_path).convert(\"RGB\")\n    \n    # 針對三個模型生成不同尺寸的圖像張量\n    img_tensor1 = test_transform_model1(img)\n    img_tensor2 = test_transform_model2(img)\n    img_tensor3 = test_transform_model3(img)\n    \n    #test_data.append((img_name, img_tensor1, img_tensor2))\n    test_data.append((img_name, img_tensor1, img_tensor2, img_tensor3))\n\n# ============================\n# 4. Ensemble 預測\n# ============================\nprint(\"Predicting with ensemble\")\nresults = []\nwith torch.no_grad():\n    for img_name, img_tensor1, img_tensor2 , img_tensor3 in tqdm(test_data, desc = \"ensemble\"):\n        # 增加 Batch 維度\n        img_tensor1 = img_tensor1.unsqueeze(0).to(device)\n        img_tensor2 = img_tensor2.unsqueeze(0).to(device)\n        img_tensor3 = img_tensor3.unsqueeze(0).to(device)\n        \n        # 獲取模型輸出\n        output1 = model1(img_tensor1)\n        output2 = model2(img_tensor2)\n        output3 = model3(img_tensor3)\n        \n        # Soft Voting: 加權平均三個模型的輸出分數\n        ensemble_output = output1 * 0.75 + output2 * 0.125 + output3 * 0.125\n        \n        # 預測最終類別\n        pred = torch.argmax(ensemble_output, dim=1).item()\n        label = reverse_label_map[pred]\n        \n        results.append({\"filename\": img_name.replace(\".jpg\", \"\"), \"label\": label})\n\n# ============================\n# 5. 保存為 CSV 文件\n# ============================\nOUTPUT_FILE = \"/kaggle/working/team_18_submission_ensemble.csv\"\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(OUTPUT_FILE, index=False)\n\nprint(f\"Ensemble 預測結果已保存到 {OUTPUT_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T15:52:29.204317Z","iopub.execute_input":"2024-12-28T15:52:29.204706Z","iopub.status.idle":"2024-12-28T16:01:25.329386Z","shell.execute_reply.started":"2024-12-28T15:52:29.204678Z","shell.execute_reply":"2024-12-28T16:01:25.328727Z"}},"outputs":[{"name":"stdout","text":"Loading models\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-1fe62e08fbe2>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Preparing test data\nPredicting with ensemble\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ensemble:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1078077d025c4110aaaed9d7921157ea"}},"metadata":{}},{"name":"stdout","text":"Ensemble 預測結果已保存到 /kaggle/working/team_18_submission_ensemble.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==========================================================================================================================\n#                                                            TTA\n# ==========================================================================================================================\n\n# ============================\n# 1. 設置參數\n# ============================\nTEST_DIR = \"/kaggle/input/2024-deep-learning-final-project/test_images\"\nOUTPUT_FILE = \"/kaggle/working/team_18_submissionTTA.csv\"\n\nMODEL_PATH = \"/kaggle/input/eva_336_0.9334/pytorch/default/1/eva336_0.933.pth\"  # 已訓練的模型檢查點\nIMAGE_SIZE = 336  # 輸入圖片大小\nmodel_name = \"eva02_large_patch14_clip_336\"  # 使用的模型名稱\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ============================\n# 2. 測試數據增強 (TTA)\n# ============================\nprint(\"Setting up TTA transforms\")\n\ntta_transforms = [\n    transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n    ]),\n    transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.RandomHorizontalFlip(p=1.0),  # 水平翻轉\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n    ]),\n    transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),  # 輕量顏色增強\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n    ]),\n    transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.RandomRotation(degrees=10),  # 旋轉 10 度\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n    ])\n]\n\n# ============================\n# 3. 加載模型\n# ============================\nprint(\"Loading model\")\nnum_classes = 15  # 類別數量\nmodel = create_model(model_name, pretrained=False, num_classes=num_classes)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device), strict=False)\nmodel = model.to(device)\nmodel.eval()\n\n# ============================\n# 4. 加載測試數據\n# ============================\nprint(\"Loading test data\")\ntest_images = sorted(os.listdir(TEST_DIR))\ntest_data = []\n\nfor img_name in test_images:\n    img_path = os.path.join(TEST_DIR, img_name)\n    img = Image.open(img_path).convert(\"RGB\")\n    test_data.append((img_name, img))\n\n# ============================\n# 5. TTA 預測\n# ============================\nprint(\"Predicting with TTA\")\nresults = []\n\nwith torch.no_grad():\n    for img_name, img in tqdm(test_data, desc = \"TTA\"):\n        outputs = []\n        for transform in tta_transforms:\n            img_tensor = transform(img).unsqueeze(0).to(device)  # 應用增強並增加批次維度\n            output = model(img_tensor)\n            outputs.append(output)\n\n        # 平均 TTA 輸出的結果\n        avg_output = torch.mean(torch.stack(outputs), dim=0)\n        pred = torch.argmax(avg_output, dim=1).item()  # 獲取最終預測類別索引\n        label = reverse_label_map[pred]\n        \n        results.append({\"filename\": img_name.replace(\".jpg\", \"\"), \"label\": label})\n\n# ============================\n# 6. 保存為 CSV 文件\n# ============================\nprint(\"Saving predictions\")\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(OUTPUT_FILE, index=False)\nprint(f\"預測結果已保存到 {OUTPUT_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T10:15:29.215317Z","iopub.execute_input":"2024-12-28T10:15:29.215616Z","iopub.status.idle":"2024-12-28T10:29:47.519551Z","shell.execute_reply.started":"2024-12-28T10:15:29.215595Z","shell.execute_reply":"2024-12-28T10:29:47.518844Z"}},"outputs":[{"name":"stdout","text":"Setting up TTA transforms\nLoading model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-21-a39d997ee345>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(MODEL_PATH, map_location=device), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"Loading test data\nPredicting with TTA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TTA:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bc37d2c015646c0a64c16036e6d995f"}},"metadata":{}},{"name":"stdout","text":"Saving predictions\n預測結果已保存到 /kaggle/working/team_18_submissionTTA.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ==========================================================================================================================\n#                                                            Ensemble + TTA(效益有限))\n# ==========================================================================================================================\n\n# ============================\n# 1. 加載模型\n# ============================\n# 參數設置\nTEST_DIR = \"/kaggle/input/2024-deep-learning-final-project/test_images\"\nOUTPUT_FILE = \"/kaggle/working/team_18_submissionTTA.csv\"\n\nMODEL_PATH_1 = \"/kaggle/input/eva_336_0.9304/pytorch/default/1/eva02_336_0.9304.pth\"\nMODEL_PATH_2 = \"/kaggle/input/eva02_224_best/pytorch/default/1/eva02_224_best.pth\"\nMODEL_PATH_3 = \"/kaggle/input/eva02_448/pytorch/default/1/eva02_448.pth\"\n\nMODEL_NAME1= \"eva02_large_patch14_clip_336\"\nMODEL_NAME2= \"eva02_large_patch14_clip_224\"\nMODEL_NAME3= \"eva02_large_patch14_448\"\n\nIMAGE_SIZE_1 = 336  # 模型1的輸入尺寸\nIMAGE_SIZE_2 = 224  # 模型2的輸入尺寸\nIMAGE_SIZE_3 = 448  # 模型3的輸入尺寸\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_model(model_name, model_path):\n    model = create_model(model_name, pretrained=False, num_classes=len(reverse_label_map))\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n    return model\n    \nprint(\"Load model......\")\n\n# 模型路徑與加載\nMODEL1_PATH = \"/kaggle/input/eva_336_0.9304/pytorch/default/1/eva02_336_0.9304.pth\"\nMODEL2_PATH = \"/kaggle/input/eva02_224_best/pytorch/default/1/eva02_224_best.pth\"\nMODEL3_PATH = \"/kaggle/input/eva02_448/pytorch/default/1/eva02_448.pth\"\n\n# ============================\n# 2. 數據增強\n# ============================\ntest_transform_model1_1 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_1, IMAGE_SIZE_1)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model1_2 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_1, IMAGE_SIZE_1)),\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model1_3 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_1, IMAGE_SIZE_1)),\n    transforms.RandomRotation(degrees=10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\ntest_transform_model2_1 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_2, IMAGE_SIZE_2)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model2_2 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_2, IMAGE_SIZE_2)),\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model2_3 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_2, IMAGE_SIZE_2)),\n    transforms.RandomRotation(degrees=10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\ntest_transform_model3_1 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_3, IMAGE_SIZE_3)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model3_2 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_3, IMAGE_SIZE_3)),\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\ntest_transform_model3_3 = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE_3, IMAGE_SIZE_3)),\n    transforms.RandomRotation(degrees=10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # For eva02_enormous_patch14_clip_224 #######\n])\n\nprint(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n#                                分析正確率\n# =============================================================================\n\nfrom sklearn.metrics import confusion_matrix\nMODEL_PATH = \"/kaggle/input/convnextv2_large/pytorch/default/1/best.pth\"  # 已訓練的模型檢查點\nIMAGE_SIZE = 384  # 輸入圖片大小\nmodel_name = \"convnextv2_large\"  # 使用的模型名稱\nprint(\"Loading model\")\n\nnum_classes = 15  # 類別數量\nmodel = create_model(model_name, pretrained=False, num_classes=num_classes)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device), strict=False)\nmodel = model.to(device)\nmodel.eval()\n\ndef evaluate_model(model, dataloader, device):\n    model.eval()  # 設置為驗證模式\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return np.array(all_preds), np.array(all_labels)\n\n# 推理並計算預測值和真實值\npreds, labels = evaluate_model(model, val_loader, device)\n\n# 計算混淆矩陣\nconf_matrix = confusion_matrix(labels, preds)\n\n# 計算每個類別的準確率\nclass_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n\n# 打印每個類別的準確率\nprint(\"\\n各類別正確率:\")\nclass_accuracy_dict = {}\nfor class_idx, accuracy in enumerate(class_accuracy):\n    class_name = reverse_label_map[class_idx]\n    class_accuracy_dict[class_name] = accuracy\n    print(f\"類別 {class_name}: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:38:33.146053Z","iopub.execute_input":"2024-12-30T08:38:33.146428Z","iopub.status.idle":"2024-12-30T08:39:20.210912Z","shell.execute_reply.started":"2024-12-30T08:38:33.146393Z","shell.execute_reply":"2024-12-30T08:39:20.209821Z"}},"outputs":[{"name":"stdout","text":"Loading model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-ffa9ec8a5726>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(MODEL_PATH, map_location=device), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"\n各類別正確率:\n類別 calling: 95.52%\n類別 clapping: 95.59%\n類別 cycling: 98.48%\n類別 dancing: 92.42%\n類別 drinking: 92.75%\n類別 eating: 97.10%\n類別 fighting: 92.31%\n類別 hugging: 97.10%\n類別 laughing: 86.36%\n類別 listening_to_music: 88.06%\n類別 running: 97.06%\n類別 sitting: 86.76%\n類別 sleeping: 100.00%\n類別 texting: 86.36%\n類別 using_laptop: 95.59%\n","output_type":"stream"}],"execution_count":5}]}